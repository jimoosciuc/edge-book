# 5.3 yurthub 源码分析



本章节将从yurthub 的架构，源代码，流程图等来分析 yurthub 的实现

具体功能

​    YurtHub是一个节点守护进程，它支持每个边缘节点的节点自治。当集群边缘网络建立时，它将必要的APIServer对象的状态缓存到本地存储中。在边缘节点离线的情况下，YurtHub保证该节点中所有注册的组件都可以访问缓存的APIServer对象状态，假装APIServer仍然是活的。

​    也就是说YurtHub提供了一种弱网环境的解决方案，即使网络不稳定造成和中心节点失去连接，健康检查依旧在进行，并且应用依然可以访问到APIServer的数据。

​    架构图摘自openyurt官网

1.架构

<img src="./image/yurthub.png" alt="img" style="zoom:80%;" />



2.组件说明

1. http server: 根据边缘节点和云端APIServer的网络情况，将所有发送到本地接口的请求代理转发到相对应的接口
2. Load balancer: 本身作为一个负载均衡的组件，转发请求到多个APIServer服务的其中一个。调度算法支持轮训，基于优先级的路由
3. Ceph Manager:提供管理本地存储数据的接口。数据将在获取到云站点的响应请求的时候更新，并且在脱机的情况下，将由 Local Proxy 读取到数据并将缓存的数据返回。如果是在线情况下，从远端读取数据
4. Certificate Manager: 为所有需要访问云站点APIServer的组件管理客户端的证书，例如，yurthub可能使用kubelet生成的证书和cloud site APIServer进行通信
5. Health Check: 周期性的检查边缘节点和云站点的网络的连接性



3.源码分析

3.1 关于目录结构的分析

➜  openyurt git:(master) ✗ tree cmd/yurthub 

```
cmd/yurthub
├── app
│   ├── config
│   │   └── config.go  YurtHub 日志的定义
│   ├── options
│   │   └── options.go  YurtHub Flag的定义及渲染
│   └── start.go  使用cobra的风格定义YurtHub的启动方法
└── yurthub.go   YurtHub 进程的入口
```

➜  openyurt git:(master) ✗ tree pkg/yurthub 

```
pkg/yurthub 
├── cachemanager 包括 cache_agent , cache_manager , storage(具体的实现在 storage/disk)
│   ├── cache_agent.go  CacheManager

│   ├── cache_agent_test.go
│   ├── cache_manager.go 定义了cache_manager接口及实现
│   ├── cache_manager_test.go
│   └── storage_wrapper.go 定义了存储的封装，具体缓存到磁盘的实现
├── certificate
│   ├── certificate.go 
│   ├── certificate_test.go
│   ├── hubself
│   │   └── cert_mgr.go  定义了YurtHub的证书管理方法
│   ├── interfaces
│   │   └── interfaces.go  定义了证书接口，包含Update,GetRestConfig,GetCaFile,NotExpired
│   └── kubelet
│       ├── cert_mgr.go  定义了kubelet的证书管理方法
│       └── cert_mgr_test.go
├── gc
│   └── gc.go  定义了清理storage的方法，一种是pod，一种是event，实现比较容易，暂时不作为主要解读
├── healthchecker
│   ├── fake_checker.go  只返回健康，若配置此模式
│   ├── health_checker.go  节点健康检查的基本方法
│   ├── health_checker_test.go
│   ├── node_lease.go  节点租约的创建，更新，备份等方法
│   └── node_lease_test.go
├── kubernetes
│   └── serializer
│       └── serializer.go  全局序列化器
├── metrics
│   └── metrics.go  定义普罗米修斯的 metrics接口
├── proxy
│   ├── local  代理到本地缓存
│   │   ├── local.go
│   │   └── local_test.go
│   ├── proxy.go  定义了反向代理的实现方案
│   ├── remote
│   │   ├── loadbalancer.go  根据负载均衡的配置，代理到符合条件的中心apiserver
│   │   ├── loadbalancer_test.go
│   │   └── remote.go  代理到中心云apiserver的实现方案
│   └── util
│       ├── util.go  抽象出去了一些基本方法，比如获取content-type
│       └── util_test.go
├── server
│   ├── certificate.go  提供证书更新接口
│   └── server.go  定义并启动 yurthub server
├── storage
│   ├── disk  定义后端存储的disk方案
│   │   ├── storage.go
│   │   └── storage_test.go
│   ├── factory
│   │   └── factory.go  目前只有disk的方案
│   └── store.go  定义了一个接口，为了以后的扩展方便
├── transport
│   └── transport.go  维护apiserver之间的连接
└── util  一些工具类
    ├── connrotation.go
    ├── util.go
    └── util_test.go


```

根据组件概况可以分析得出结论，yurthub由3部分构成

1.是南北向流量控制管理，包括证书管理，负载均衡，健康检查

2.是东西向流量管理，包括HttpServer（拦截其他需要访问站点apiserver的组件）,以及针对

3.是本地缓存cache，包括 CacheManager组件，Storage Manager组件

![yurthub-sub](./image/yurthub-subs.png)

下面依次分析以上三部分，结合具体代码流程来讲解实现

1.南北向流量控制

反向代理模块：



定义：

```go
type yurtReverseProxy struct {
	resolver            apirequest.RequestInfoResolver
	loadBalancer        remote.LoadBalancer
	localProxy          *local.LocalProxy
	cacheMgr            cachemanager.CacheManager
	maxRequestsInFlight int
	stopCh              <-chan struct{}
}

```

从定义可以推断 本接口包含一个解析器，一个外部负载均衡器，一个本地代理，一个缓存管理器（用于管理本地缓存），maxRequestsInFlight用于控制最大的代理请求数量，如果超过这个限制就拒绝请求。maxRequestsInFlight默认大小为1 << 20



转发：

```go
func (p *yurtReverseProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {
   if !hubutil.IsKubeletLeaseReq(req) && p.loadBalancer.IsHealthy() {
      p.loadBalancer.ServeHTTP(rw, req)
   } else {
      p.localProxy.ServeHTTP(rw, req)
   }
}
```

从这个方法可以得知yurtReverseProxy结构体是一个http.Handler对象，此方法中包含一个判断条件，判断是否是来自kubelet,的leases请求，若是则走本地proxy，另外就是判断和远端的负载均衡组件之间的链接是否健康，若不是健康则走本地proxy。否则走远端。

下面分别看下远端和本地proxy是如何工作的。

```go
handler = util.WithRequestTrace(handler)
handler = util.WithRequestContentType(handler)
handler = util.WithCacheHeaderCheck(handler)
handler = util.WithRequestTimeout(handler)
handler = util.WithListRequestSelector(handler)
handler = util.WithRequestClientComponent(handler)
handler = filters.WithRequestInfo(handler, p.resolver)
handler = util.WithMaxInFlightLimit(handler, p.maxRequestsInFlight)
```

这里的几个关于http.Handler的方法，构成了一个链。主要是对请求和响应做一个拦截校验的工作

WithRequestTrace用于记录整个请求经历的时间，统计当前所有的请求数，记录响应码。

WithRequestContentType用于检查头部有没有设置Accept,以及有没有设置内容将直接拦截拒绝服务（此处代码和原代码注释的含义不同，我按照代码来写文章,目前已经提交并合入了PR，更正了这个注释）

WithCacheHeaderCheck用于检查头部有没有带有Edge-cache,若带有并且等于 true，则修改ctx中的canCache标记位，并且将头部Edge-cache删除(这一步我的理解是安全)

WithRequestTimeout用于对不同种类的请求做超时时间的调整，针对watch request,timeout被设置为指定的(timeout+15)s,这样做的目的是当watch结束后，取消丢失了来自远端kube-apiserver的断开信号的请求；针对get/list接口的timeout被设置为指定的(timeout-2)s,当远端的服务失败后可以在客户端超时之前从cache中获取数据（这部分在超时之前从cache获取数据没有在代码中体现，需要后面关注下是否有遗漏部分）



后面会通过NewYurtHubServer方法构造一个Server，该Server包含两个http.Server,分别是hubServer,proxyServer,yurthubserver提供了动态追踪，健康检查，更新token等接口。动态追踪接口主要是使用了pprof下的若干包及方法。健康检查直接返回ok。metric是调用的promhttp.handler()调用的普罗米修斯的默认方法。

启动流程：

```go
// Run runs the YurtHubConfiguration. This should never exit
func Run(cfg *config.YurtHubConfiguration, stopCh <-chan struct{}) error 
```

yurthub的启动流程为 

1.初始化 transportManager

2.创建春初管理器

3.创建序列化管理器

4.创建缓存管理器

5.创建证书管理器，并注册kubelet,hubself

6.刷新transportManager的证书

7.启动健康检查模块

8.启动清理模块

9.初始化声明反向代理模块

10.初始化yurthubserver，并启动yurthubserver

11.用不停止



TODO：yurthub新增了虚拟网卡，是最近刚合入的新特性，放到后面分析。

https://github.com/openyurtio/openyurt/commit/84d3fe8f3e6b549cb53bc97e8abf74f7077823db